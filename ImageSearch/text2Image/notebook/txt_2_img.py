# -*- coding: utf-8 -*-
"""txt_2_img.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r7I2ytCihra6QNEYvChxh5tBD0hS8f00

Sentence Transformer:
SentenceTransformers ðŸ¤— is a Python framework for state-of-the-art sentence, text and image embeddings.

[Hugging Face](https://huggingface.co/sentence-transformers)

[SentenceTransformers Documentation](https://www.sbert.net/)

# Important:
[Usage (Sentence-Transformers)](https://huggingface.co/sentence-transformers/clip-ViT-B-32-multilingual-v1)

[Image Search](https://www.sbert.net/examples/applications/image-search/README.html)
"""

!pip install sentence-transformers

"""# Dataset:

[Roboflow](https://universe.roboflow.com/roboflow-100/animals-ij5d2/dataset/2)
"""

data_path = "/content/drive/MyDrive/Colab_Notebooks/GenerativeAI/CLIP/text_to_image/animals/"

import os
os.listdir(data_path)

from sentence_transformers import SentenceTransformer, util
from PIL import Image
import glob
import torch
import pickle
import zipfile
from IPython.display import display
from IPython.display import Image as IPImage
import os
from tqdm.autonotebook import tqdm

import numpy as np
import matplotlib.pyplot as plt

"""Model:

https://huggingface.co/sentence-transformers/clip-ViT-B-32
"""

model = SentenceTransformer('clip-ViT-B-32')

"""# Image embeddings."""

img_model = SentenceTransformer('clip-ViT-B-32')

img_names = list(glob.glob(f'{data_path}*.jpg'))

print("Images:", len(img_names))

"""[Properties and methods when you load a SentenceTransformer model](https://www.sbert.net/docs/package_reference/SentenceTransformer.html)

# Map images to the vector space
"""

img_emb = img_model.encode(
                            [Image.open(filepath) for filepath in img_names],
                            batch_size=128,
                            convert_to_tensor=True,
                            show_progress_bar=True
                            )

img_emb.shape

type(img_emb)

"""# Search function."""

def plot_images(images, query, n_row=2, n_col=2):
    _, axs = plt.subplots(n_row, n_col, figsize=(12, 12))
    axs = axs.flatten()

    for img, ax in zip(images, axs):
        ax.set_title(query)
        ax.imshow(img)

    plt.savefig('foo.png')
    plt.show()

"""# Compute cosine similarities:"""

def search(query, k=4):
    # First, we encode the query (which can either be an image or a text string)
    query_emb = model.encode([query],
                             convert_to_tensor=True,
                             show_progress_bar=False)

    # Then, we use the util.semantic_search function, which computes the cosine-similarity
    # between the query embedding and all image embeddings.
    # It then returns the top_k highest ranked images, which we output
    hits = util.semantic_search(query_emb, img_emb, top_k=k)[0]
    print(hits)

    matched_images = []
    for hit in hits:
        matched_images.append(Image.open(img_names[hit['corpus_id']]))

    plot_images(matched_images, query)
    # print(matched_images)

"""person,chicken,cow,goat,horse,dog,racoon,cat,fox,skunk"""

search("Picture of Goat")

